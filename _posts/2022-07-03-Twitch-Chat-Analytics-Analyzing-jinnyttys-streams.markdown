---
# multilingual page pair id, this must pair with translations of this page. (This name must be unique)
lng_pair: id_autogeneratedsamplecontent_1
title: Twitch Chat Analytics Analyzing jinnytty's streams

# post specific
# if not specified, .name will be used from _data/owner.yml
#author: ""
# multiple category is not supported
category: cyber-security
# multiple tag entries are possible
tags: ["Pandas"]
# thumbnail image for post
img: ":insider_threat_program.jpg"
# disable comments on this page
comments_disable: true

# publish date
date: 2022-07-03 17:00:00 +0600

# seo
# if not specified, date will be used.
#meta_modify_date: 2021-08-10 11:32:53 +0900
# check the meta_common_description in _data/lang/[language].yml
#meta_description: ""

# optional
# if you enabled image_viewer_posts you don't need to enable this. This is only if image_viewer_posts = false
#image_viewer_on: true
# if you enabled image_lazy_loader_posts you don't need to enable this. This is only if image_lazy_loader_posts = false
#image_lazy_loader_on: true
# exclude from on site search
#on_site_search_exclude: true
# exclude from search engines
#search_engine_exclude: true
# to disable this page, simply set published: false or delete this file
#published: false
---

# Twitch Chat Analytics: Analyzing jinnytty's streams

## Context
Yoo Yoonjin (Korean: 유윤진; born July 28, 1992) better known as Jinnytty, Yoo is best known for her IRL Twitch livestreams who has been streaming for 5 years
From 2020 to 2022, Yoo traveled to and streamed live in more than 20 countries in different continents like Asia, Europe and north america.

## Personal Motivation

Twitch community it is well know as tech savvy so i decided to develop the analysis in this community because they could give me a good feedback about
the report and coding and also jinnytty is one of my favorites streamers

## Business Task

There isn't a business task I mainly did this project to improve my skill using pandas and their packages and the process of developing a BI analytics

## Key Stakeholders
so i said before this is personal project but through the endeavor doing it i found some people that is interesting with it

## Preparation and first cleaning

Twitch is a website focus it on streaming although they had develop their chat with the Internet Relay Chat (IRC) protocol also you can access their database with an api
knowing that we are going to use 2 tools for getting chat logs [RechatTool](https://github.com/jdpurcell/RechatTool) and [Chatterino](https://chatterino.com/)

None of them provide data and timestamp so I'll explain who do the data for each one separately.

---
### RechatTool

1- The first cleaning that i do it is done it with bash and is automated 

`#!/bin/bash`

2- I find every Quotation mark and delete then. i do this because a quotation mark could interfere when i am running some code in the future

`sed 's/"//g' *.txt > withoutcomillas.txt &&`

3- I use verticals bars as separators so this symbol could make another column when i am reading the file with pandas 

`sed 's/|//g' withoutcomillas.txt > datawithits.txt &&`

4- Because this analysis is about twitch and the main way to express yourself its with emotes i need to normalize all world  with apostrophe

`sed -r "s/It’s/its/g" datawithits.txt | sed -r "s/It’s/its/g" | sed -r "s/That’s/Thats/g" | sed -r "s/M&M's/MMs/g" > yyjdata.txt | sed -r "s/don't/dont/g" > yyjdata.txt &&`

5- Separating the data in different files will makes easy the cleaning at the end i will merge then

`awk '{print $1}' yyjdata.txt | awk '{print substr($0,2,8);}' > time.csv &&`

6- Separating the data in different files will makes easy the cleaning at the end i will merge then

`awk '{print $2}' yyjdata.txt | awk -F: '{print $1}' > user.csv &&`

7- Separating the data in different files will makes easy the cleaning and at the end i will merge then

`awk -F: '{ for(i=1; i<=3; i++){ $i="" }; print $0 }' yyjdata.txt | awk '{print substr($0, 5, length($0))}' > messages.csv &&`

8- Merging all the files into one

`paste -d '\|' time.csv user.csv messages.csv > readydata.txt &&`

9- At the start of the stream is always played an intro and some user spam and that makes the analysis biased so I need to find when the intro finished and that is easy because usually, a bot sent a message saying that scene switched to live so i find that row and everything before that it is deleted

`sed '1,/super_stream_server|Scene switched to  Live/d' readydata.txt > awkcleaning.txt  &&`

10- Then i deleted some rows with text strings that spawn nan values in the last column

`awk '!/just earned/ &&  !/sending messages too quickly/ && !/emote-only/ && !/You can find your currently available/ && !/raiders from/ && !/redeemed/ && !/streamelements/ && !/innytty is live!/ && !/StreamElements/`

11- I delete the rest of the files

`rm datawithits.txt time.csv user.csv messages.csv withoutcomillas.txt yyjdata.txt readydata.txt awkcleaning.txt`

**Next i open the file with pandas because we need to add the timestamp to every row**

`import pandas as pd`
`df = pd.read_csv('../yyj.csv', delimiter='|', encoding='utf8', header=None, names=["Time", "User", "Message"])`

1- Sometimes i do the analysis the next day that the stream was streamed so i need to subtract one day

`df["Time"] = pd.to_datetime(df["Time"]) - + pd.Timedelta(days=1)`

2- Here i will add the timestamp 

`df["Time"] = pd.to_datetime(df["Time"]) + pd.Timedelta(hours=6, minutes=3)`

3- next i separate the date from the timestamp in one column

`df["Day"] = pd.to_datetime(df["Time"]).dt.strftime('%Y-%m-%d')`

4- and then i do the same with timestamp

`df["Time"] = pd.to_datetime(df["Time"]).dt.strftime('%H:%M:%S')`

5- i organize the columns and the file its ready for the analysis 

`df = df[["Day", "Time", "User", "Message"]]`
`df.to_csv('../yyj.txt',index=False, header=False, sep='|')`

---
### Chatterino

With Chatterino the cleaning process it is similar to RechatTool although we need to do some extra steps

1- merge files with date /usr/bin/merge

`for i in find . -name "*.log" -type f; do`
`    ls $i | xargs -I{} awk '{print "{}", $0}' $i`
`done`

2- Cleaning the ""

`sed 's/"//g' data.txt > withoutcomillas.txt &&`

3- Cleaning the |

`sed 's/|//g' withoutcomillas.txt > datawithits.txt &&`

4- Cleaning It's and it's 

`sed -r "s/It’s/its/g" datawithits.txt | sed -r "s/It’s/its/g" | sed -r "s/That’s/Thats/g" | sed -r "s/M&M's/MMs/g" > yyjdata.txt &&`

5- Clean the first column and export only the data in a new file

`awk '{print substr($1,12,10); }' yyjdata.txt > a.txt &&`

6- Take the column of time to another file

`awk '{print $2}' yyjdata.txt > time.txt &&`

7- Removing the 1st and last character of every line "[]" from the file time.txt then export the data to b.txt

`sed 's/.//;s/.$//' time.txt > b.txt &&`

8- Deleting the last character from the third column ":"

`awk -F: '{if (NR!=0) {print substr($3, 6, length($3))}}' yyjdata.txt > c.txt &&`

9- extracting messages

`awk -F: '{ for(i=1; i<=3; i++){ $i="" }; print $0 }' yyjdata.txt > mssgs.txt &&`

10- Deleting spaces from mssgs

`awk '{print substr($0, 5, length($0))}' mssgs.txt > d.txt &&`

11- Merging the files a, b, and c into one file

`paste -d '\|' a.txt b.txt c.txt d.txt > readydata.txt &&`

12- starting stream

`sed '1,/super_stream_server|Scene switched to  Live/d' readydata.txt > awkcleaning.txt &&`

13- search all lines that contains # and delete them

`awk '!/just earned/ &&  !/sending messages too quickly/ && !/emote-only/ && !/You can find your currently available/ && !/raiders from/ && !/redeemed/ && !/streamelements/ && !/innytty is live!/' awkcleaning.txt > yyj.txt`

Removing the files a.txt b.txt c.txt d.txt

`rm a.txt b.txt c.txt d.txt data.txt yyjdata.txt time.txt readydata.txt awkcleaning.txt mssgs.txt withoutcomillas.txt datawithits.txt`

### Loading dependencies and data

Now that the data is organized we can start handle it with pandas

```
import pandas as pd
from matplotlib import dates as mpl_dates 
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd 
import seaborn as sns
from scipy import stats
from textblob import TextBlob
import math
from datetime import datetime, timedelta 
from jinja2 import Environment, FileSystemLoader

read = pd.read_csv('june.txt', delimiter=',', encoding='utf8', header=None, names=["day", "time", "user", "message"])
```
<img src="https://i.imgur.com/FuYn3rQ.jpg" style="margin-left: 5%" >

### Summary 

<img src="https://i.imgur.com/tirXLKV.jpg" style="margin-left: 5%" >

## Normalizing and cleaning the data 

We already did a lot of cleaning before but we need to take extra steps

Because we are using two sources they record the user names different we are going to standardize the data 

First all user names need to be in lowercase

`df['user'] = df['user'].str.lower()`

Then replace the user who has two record for their name to only one

`df["user"].replace({"센트23 vincentt23":"빈센트23", "루이스와이푸 haruiswaifu":"하루이스와이푸", "雞力 winterreise1988":"愛雞力", "죠우":"코죠우"}, inplace=True)`

The last step is deleting all NA values 

`clean_wothoutNA = df.dropna()`

## Processing the data

### identifying which emotes where more used

As i said before twitch community like to use emotes to express their self so it is really interesting to see which emote was used more i will do this analysis first by taking the data frame without nam values
then take each word of each row separating them and then group each one for counting them.

I will take only the first seventy words and then i apply a filter that only leaves emotes and not English words 

<img src="https://i.imgur.com/SRWP0kE.jpg" style="margin-left: 5%" >

The outcome

<img src="https://i.imgur.com/1MX67W5.jpg" style="margin-left: 5%" >

### Top chatters with their most used emote

The first times i did the analysis and show to some friends they ask me which emote they use more and how many times they send a messages 
I found that the most interested people in the analysis is usually the user who sends more messages per stream 

<img src="https://i.imgur.com/oYDL9BR.jpg" style="margin-left: 5%" >

Outcome 

<img src="https://i.imgur.com/MLCGVqr.jpg" style="margin-left: 5%" >

### Total messages per day

Finding which day has more interactions 

<img src="https://i.imgur.com/0V34vjy.jpg" style="margin-left: 5%" >

### Total user per day

A similar process as i did before but this time I will find how many users were in each day 

<img src="https://i.imgur.com/Ixk0ivs.jpg" style="margin-left: 5%" >

### Total user and messages per day graphs 


```
chart_messeges_per_day = plt.figure(figsize=(30,10))
chart_messeges_per_day = plt.plot(newplot.Day, newplot.Messages, "g.-")
chart_messeges_per_day = plt.title("Messages per day")
chart_messeges_per_day = plt.xlabel('Days.', fontsize=18)
chart_messeges_per_day = plt.ylabel('Messages.', fontsize=16)
chart_messeges_per_day = plt.xticks(fontsize=12, rotation=360)
chart_chatters_per_day = plt.yticks(fontsize=13)
chart_messeges_per_day = plt.show()

```

<img src="https://i.imgur.com/Yv2ZLdw.jpg" style="margin-left: 5%" >

```
chart_chatters_per_day = plt.figure(figsize=(30,10))
chart_chatters_per_day = plt.plot(newplot.Day, newplot.Chatters, "b.-")
chart_chatters_per_day = plt.title("Unqiue chatters per day")
chart_chatters_per_day = plt.xlabel('Days.', fontsize=18)
chart_chatters_per_day = plt.ylabel('Unique chatters.', fontsize=18)
chart_chatters_per_day = plt.xticks(fontsize=12, rotation=360)
chart_chatters_per_day = plt.yticks(fontsize=13)
chart_chatters_per_day = plt.show()
```

<img src="https://i.imgur.com/5uZyIED.jpg" style="margin-left: 5%" >

